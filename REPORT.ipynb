{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Can we predict , within 10 years, whether or not a person sees the risk to get coronary heart disease?***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    " 1. Initialization\n",
    " 2. Show our dataset\n",
    " 3. Data Exploration\n",
    " 4. Modify and work on the dataset \n",
    " 5. Prediction with Logistic Regression\n",
    "  - Confusion Matrix\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-Score\n",
    "  - Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)\n",
    "  - Accuracy\n",
    " 6. Prediction with K-Nearest Neighbour\n",
    "  - Confusion Matrix\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-Score\n",
    "  - Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)\n",
    "  - Accuracy\n",
    " 7. Prediction with Supported Vector Machine\n",
    "  - Confusion Matrix\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-Score\n",
    "  - Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)\n",
    "  - Accuracy\n",
    " 8. Prediction with Neural Network\n",
    "  - Confusion Matrix\n",
    "  - Precision\n",
    "  - Recall\n",
    "  - F1-Score\n",
    "  - Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)\n",
    "  - Accuracy\n",
    "  - Loss Curve\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "World Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases; in fact, Cardiovascular diseases are the number **1** cause of death globally!\n",
    "The early predictions of cardiovascular diseases can make lifestyle changes in high risk patients, and it can reduce the complications.\n",
    "This project intend to prove the correlation between current behaviours of a person, and his future risk of heart disease, using --models--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "from termcolor import colored\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metric\n",
    "from matplotlib import pyplot\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we:\n",
    " - load our dataset\n",
    " - delete rows with a NaN value inside 'heartRate','cigsPerDay','BMI','glucose','BPMeds','totChol'\n",
    " - substitute NaN values inside 'education' with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/framingham.csv', header='infer', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#delete rows with a NoNe value that is in 'heartRate','cigsPerDay','BMI','glucose','BPMeds','totChol'\n",
    "columns_null=['heartRate', 'cigsPerDay', 'BMI', 'glucose', 'BPMeds', 'totChol']\n",
    "for column in columns_null:\n",
    "    df.drop(df[df[column].isna()==True].index, inplace=True)\n",
    "    \n",
    "    \n",
    "#in 'education', substitute NaN values with 0\n",
    "df['education'] = df['education'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Show our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consist of an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts.\n",
    "\n",
    "We have a dataset consisting of **3749 rows** and **16 columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographic:\n",
    "- Male:\n",
    " - 1 if male \n",
    " - 0 if female\n",
    "\n",
    "\n",
    "- Age: age of the patient in range (32,70)\n",
    "\n",
    "#### Behavioral\n",
    "- Education:\n",
    " - 0 if unknown\n",
    " - 1 = Some High School\n",
    " - 2 = High School or GED\n",
    " - 3 = Some College or Vocational School\n",
    " - 4 = college and further\n",
    " \n",
    " \n",
    "- Current Smoker: \n",
    "  - 1 if the patient is a current smoker \n",
    "  - 0 if the patient is NOT a current smoker \n",
    "  \n",
    "\n",
    "- Cigs Per Day: the number of cigarettes that the person smoked on average in one day\n",
    "\n",
    "#### Medical (history)\n",
    "- BP Meds: \n",
    "  - 1 if the patient is on blood pressure medication\n",
    "  - 0 if the patient is NOT on blood pressure medication\n",
    "  \n",
    "  \n",
    "- Prevalent Stroke: \n",
    "  - 1 if the patient had a stroke previously\n",
    "  - 0 if the patient had NOT a stroke previously\n",
    "  \n",
    "  \n",
    "- Prevalent Hyp:\n",
    "  - 1 if the patient is hypertensive \n",
    "  - 0 if the patient is NOT hypertensive\n",
    "  \n",
    "  \n",
    "- Diabetes: \n",
    "  - 1 if the patient has diabetes \n",
    "  - 0 if the patient has NOT diabetes \n",
    "  \n",
    "  \n",
    "#### Medical(current)\n",
    "- Tot Chol: total cholesterol level\n",
    "\n",
    "\n",
    "- Sys BP: systolic blood pressure \n",
    "\n",
    "\n",
    "- Dia BP: diastolic blood pressure\n",
    "\n",
    "\n",
    "- BMI: Body Mass Index\n",
    "\n",
    "\n",
    "- Heart Rate: heart rate\n",
    "\n",
    "\n",
    "- Glucose: glucose level \n",
    "\n",
    "\n",
    "#### Predict variable (desired target)\n",
    "- TenYearCHD: 10 year risk of coronary heart disease CHD\n",
    "  - 1 if “Yes”\n",
    "  - 0 if “No”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually we can quite demonstrate that , for some of the variables, our dataset is enough balanced... but for other variables the dataset is NOT balanced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1<sup>st</sup> example: SMOKERS vs NON-smokers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we will notice that **current smokers** (and therefore also **current non-smokers**) cover almost half of the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "countNoSmoker = len(df[df.currentSmoker == 0])\n",
    "countSmoker = len(df[df.currentSmoker == 1])\n",
    "print(colored(\"Percentage of Current NON-Smoker Patients: {:.2f}%\".format((countNoSmoker / (len(df.currentSmoker)) *100)), 'green', attrs=['bold']))\n",
    "print(colored(\"Percentage of Current Smoker Patients: {:.2f}%\".format((countSmoker / (len(df.currentSmoker))*100)), 'green', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2<sup>nd</sup> example: patients WITH diabetes vs withOUT diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*But sometime it doesn't happen!*\n",
    "\n",
    "*For example, the percentage of patients with and without ***diabetes*** is not balanced:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "diabetes0 = len(df[df.diabetes == 0])\n",
    "diabetes1 = len(df[df.diabetes == 1])\n",
    "print(colored(\"Percentage Patients WITH Diabetes: {:.2f}%\".format((diabetes1 / (len(df.diabetes))*100)), 'green', attrs=['bold']))\n",
    "print(colored(\"Percentage Patients withOUT Diabetes: {:.2f}%\".format((diabetes0 / (len(df.diabetes))*100)), 'green', attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*In fact, as for the diabetes, we can note as well an imbalance about ***the prediction of the risk of coronary heart disease CHD within 10 years***:*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3<sup>rd</sup> example: patients WITH or withOUT risk of coronary heart disease CHD within 10 years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target0 = len(df[df.TenYearCHD == 0])\n",
    "target1 = len(df[df.TenYearCHD == 1])\n",
    "print(colored(\"Percentage of Patients withOUT risk of coronary heart disease CHD within 10 years: {:.2f}%\".format((target0 / (len(df.TenYearCHD))*100)),'green',attrs=['bold']))\n",
    "print(colored(\"Percentage of Patients WITH risk of coronary heart disease CHD within 10 years: {:.2f}%\".format((target1 / (len(df.TenYearCHD))*100)),'green',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4<sup>th</sup> example: frequency of a previous Stroke differentiated for Sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see another example to show the balancing of the dataset, seeing the ***frequency of a previous Stroke differentiated for Sex***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.male, df.prevalentStroke=='1').plot(kind=\"bar\", figsize=(12, 7), color=['#AA1111'])\n",
    "plt.title('Frequency of a previous Stroke for Sex', fontsize=20)\n",
    "plt.xlabel('Sex:\\n  0 = Female\\n1 = Male')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend([\"had a stroke previously\"])\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modify and work on the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solve the inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do now is to reduce our dataset, in order to have **equality** between 0 and 1 belonging to the 'TenYearCHD' column; we'll delete some row representing patients whose risk is  $=0$  (that is, pationt witOUT risk of coronary heart disease CHD within 10 years), using 'sample' function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "shortest = min(len(df[df['TenYearCHD'] == 0]), len(df[df['TenYearCHD'] == 1]))\n",
    "\n",
    "if len(df[df['TenYearCHD'] == 1]) == shortest:\n",
    "    keep = df.loc[df[df['TenYearCHD'] == 0].sample(shortest).index, :]\n",
    "    df = pd.concat([df[df['TenYearCHD'] == 1], keep], axis=0, copy=False)\n",
    "else:\n",
    "    keep = df.loc[df[df['TenYearCHD'] == 1].sample(shortest).index, :]\n",
    "    df = pd.concat([df[df['TenYearCHD'] == 0], keep], axis=0, copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target0 = len(df[df.TenYearCHD == 0])\n",
    "target1 = len(df[df.TenYearCHD == 1])\n",
    "print(colored(\"Percentage of Patients withOUT risk of coronary heart disease CHD within 10 years: {:.2f}%\".format((target0 / (len(df.TenYearCHD))*100)),'green',attrs=['bold']))\n",
    "print(colored(\"Percentage of Patients WITH risk of coronary heart disease CHD within 10 years: {:.2f}%\".format((target1 / (len(df.TenYearCHD))*100)),'green',attrs=['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization in range (0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the choices we took to develop this project, is to divide our dataset into 2 parts:\n",
    "  -  a **training** dataset ('train_set.csv'), that is the **80%** of our dataset\n",
    "  - a **test** dataset ('test_set.csv'), that is the **20%** of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So at first we must normalize each value of those columns which values are in a too large range (we need that all the values are in the range **(0,1)**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columns_to_normalize=['age', 'cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
    "for column in columns_to_normalize:\n",
    "    df[column]=MinMaxScaler(copy=False).fit_transform(df[[column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create the ***training*** dataset and the ***test*** dataset using '***train_test_split***', a function in Sklearn model selection for splitting data arrays into two subsets: for training data and for testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRAINING dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new TRAINING dataset will consist of **915 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('datasets/train_set.csv', header='infer',encoding='utf-8')\n",
    "df_train.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new TEST dataset will consist of **229 rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('datasets/test_set.csv', header='infer',encoding='utf-8')\n",
    "df_test.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction with ***Logistic Regression***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the prediction models that we implemented is Logistic Regression (we are using \"logistic_regression.pkl\", a file inside \"models\" folder of our repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path=Path('models/logistic_regression.pkl')\n",
    "with open(path,'rb') as file:\n",
    "    model=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the **X_test** as our test-dataset with all the features, without the target column 'TenYearCHD': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test=df_test.copy(deep=True)\n",
    "X_test.drop('TenYearCHD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our target column will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target=df_test['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the **predicted output**, using the predict() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and calculate the Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(df_test['TenYearCHD']), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('TRUE label')\n",
    "plt.xlabel('PREDICTED label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),ha='center',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps us to find the True Positives, True Negatives, False Positives an False Negatives of the prediction; in fact we can visualize:\n",
    " - rows as the **true** labels\n",
    " - coumns as the **predict** labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we obtained:\n",
    " - 69 **true** Positive predictions (that is, the prediction says that a patient RUNS the risk, and he **really** runs the risk!)\n",
    " - 83 **true** Negative predictions (that is, the prediction says that a patient DOESN'T run the risk, and he **really** DOESN'T run the risk!)\n",
    " - 39 **false** Positive predictions (that is, the prediction says that a patient RUNS the risk, but he **doesn't** really run the risk!)\n",
    " - 38 **false** Negative predictions (that is, the prediction says that a patient DOESN'T run the risk, but he **really** run the risk!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision=metric.precision_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The percentage of the accuracy of our positive predictions, represented by the Precision, is: ',\"{:.2%}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall=metric.recall_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The ratio of positive instances that are correctly detected by the classifier (true positive rate), represented by the Recall, is: ',\"{:.2%}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score=metric.f1_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The F1-Score, is: ',\"{:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It represents the **harmonic** mean of precision and recall.\n",
    "Instead of the regular mean (that gives equal weight to all values), harmonic mean gives more weight to low values, \n",
    "favoring classifiers that have similar precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the **predicted probabilities of getting the output as 1**, using the predict.proba() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate ROC and AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc=roc_auc_score(np.array(df_test['TenYearCHD']), predictions)\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), predictions)\n",
    "plt.plot(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Logistic regression')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "print(' The Area Under the ROC curve (AUC) is: ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curve, we are showing the ***true positive rate*** (recall) against the ***false positive rate*** (ratio of $0$ instances that are incorrectly classified as $1$, corresponding to $1 - specificity$ , where the specificity corresponds to the true negative rate, which is the ratio of $0$ instances that are correctly classified as $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ROC curve seems to suggest that the classifier is not so so bad... but we can extract more informations seeing the value of the AUC; \n",
    "in fact, we can accept AUC values that lie between 0.5 to 1, where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.\n",
    "\n",
    "Here, our Area Under the ROC Curve is equal to almost $0.72$ , so we can say **in general** that we obtained an acceptable discrimination... but **specifically** in contexts such as our case (medical diagnosis), very high (and higher than the result we obtained) AUCs are sought. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end, we can say that the overall predicted **accuracy** of the model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The overall predicted accuracy of the model is: ',\"{:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.  Prediction with ***K-Nearest Neighbors***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another prediction models that we implemented is K-Nearest Neighbors model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('models/KNN.pkl')\n",
    "with open(path,'rb') as file:\n",
    "    model=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our predictions on the KNN model we first load the TEST dataset and then we differentiate the feautres (The first fifteen columns) with the target (TenYearCHD column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"datasets/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test=df_test.copy(deep=True)\n",
    "X_test.drop('TenYearCHD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_test=df_test['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for Logistic Regression we start with the predictions output through the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(df_test['TenYearCHD']), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this predictive model we get the following Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('TRUE label')\n",
    "plt.xlabel('PREDICTED label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),ha='center',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot rapresent the best view for the Confusion matrix. It's used for evaluating the performance of a classification model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making. In this plot we can understand An equal value for the False Positive and False Negative, this will justify the equal values calculated for the Recall, Precision and F1-Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision=metric.precision_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The percentage of the accuracy of our positive predictions, represented by the Precision, is: ',\"{:.2%}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall=metric.recall_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The ratio of positive instances that are correctly detected by the classifier (true positive rate), represented by the Recall, is: ',\"{:.2%}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score=metric.f1_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The F1-Score, is: ',\"{:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the F1 Score as well, you can see how Precision, Recall and F1 Score have the same result. As previously mentioned, the values obtained in the Confusion Matrix (FN = FP) are fundamental to justify the values obtained for the Recall, Precision, F1- Score. Starting with Recall and Precision, I know by definition that:\n",
    "\n",
    "Precision 𝑃 = $\\frac{𝑇𝑃}{𝑇𝑃+𝐹𝑃}$\n",
    "\n",
    "It represents the classifier’s ability to only predict really positive samples as positive. \n",
    "\n",
    "Recall 𝑅= $\\frac{𝑇𝑃}{𝑇𝑃+𝐹𝑁}$\n",
    "\n",
    "It represents the amount of positive test samples that were actually classified as positive.\n",
    "\n",
    "Comparing the two operations we can see that the only element of difference concerns the type of False grip. In Precision the False Positive are considered while in the Recall the False Negatives. In our case FP and FN are the same for this reason Precision and Recall give equal values.\n",
    "\n",
    "F1-Score is just the weighted average between precision and recall. But if Precision and recall are the same at the same time, F1-Score will also be the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"datasets/train_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train=df_train.copy(deep=True)\n",
    "X_train.drop('TenYearCHD',axis=1,inplace=True)\n",
    "target_train =df_train['TenYearCHD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_range = range(1, 21)\n",
    "\n",
    "f1_score = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, target_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    f1_score.append(metric.f1_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, f1_score)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Testing F1-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precisely on the F1-Score we concentrated trying to understand which neighbor value could bring an optimal F1-Score. We decided to vary the neighbor value in a range from 1 to 21 and for each value we applied our predictive model defining F1-Score. We then represented the F1-Score trend as K.\n",
    "\n",
    "Analyzing the graph we can see how the best F1-Score is obtained for a reduced and limited K value between 3 and 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The overall predicted accuracy of the model is: ',\"{:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each K from 1 to 20 we have obtained the accuracy value which we have subsequently inserted in a list and represented graphicallyOnce we calculated the accuracy score for k fixed equal to \"\", also in this case we decided to graphically represent the trend of the Accuracy Score as K varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_range = range(1, 21)\n",
    "\n",
    "accuracy_value = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, target_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy_value.append(metric.accuracy_score(target_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(k_range, accuracy_value)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Testing Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case the choice of the k value, to obtain the best Accuracy Score equal to about 0.62, falls into a low and limited range. Interval that we can consider equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc=roc_auc_score(np.array(df_test['TenYearCHD']), predictions)\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), predictions)\n",
    "plt.plot(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic (ROC) - KNN')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "print(' The Area Under the ROC curve (AUC) is: ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC Curve made for the KNN model compared to the other ROC curves is less fragmented and slightly linear. Also in this case the curve is located above the diagonal and this allows us to accept the model created. In this case the AUC value is equal to about 65%, lower than models like Neural Networks, therefore we have 65% chance that model will be able to distinguish between positive class and negative class. Analyzing the first part of the ROC Curve we realize how for very small values of TRUE Positive Rate and FALSE Positive Rate the ROC Curve concide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Predictions with ***Support Vector Machine (SVM)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another prediction models that we implemented is Support Vector Machine model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('models/svm.pkl')\n",
    "with open(path,'rb') as file:\n",
    "    model=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our predictions on the KNN model we first load the TEST dataset and then we differentiate the feautres (The first fifteen columns) with the target (TenYearCHD column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"datasets/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test=df_test.copy(deep=True)\n",
    "X_test.drop('TenYearCHD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_test=df_test['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for Logistic Regression we start with the predictions output through the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(df_test['TenYearCHD']), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('TRUE label')\n",
    "plt.xlabel('PREDICTED label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),ha='center',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also represented the Confusion Matrix for the SVM (Supported vector machine) model. We can note also in this case, as for the Neural Network, the False Positive and False Negative are not the same this will lead to different values in the definition of Precision and Recall. Compared to the previous models, the SVM model is more accurate having a higher value than TRUE (Negative and Positive) and at the same time lower False Positive and False Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Precision value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision=metric.precision_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The percentage of the accuracy of our positive predictions, represented by the Precision, is: ',\"{:.2%}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a direct consequence of the Confusion Matrix analysis we find higher precision and recall values than models like KNN, Neural Networks. This is because both False Positive (for Precison) and False Negative (for Recall) are lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recall value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall=metric.recall_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The ratio of positive instances that are correctly detected by the classifier (true positive rate), represented by the Recall, is: ',\"{:.2%}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score=metric.f1_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The F1-Score, is: ',\"{:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The overall predicted accuracy of the model is: ',\"{:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc=roc_auc_score(np.array(df_test['TenYearCHD']), predictions)\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), predictions)\n",
    "plt.plot(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Supported Vector Machine')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "print(' The Area Under the ROC curve (AUC) is: ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC Curve made for the SVM model has a very positive trend in fact it is considerably above the diagonal. Comparing the AUC values with respect to the various models proposed by us, the SVM model presents one of the highest values, about 72% chance that model will be able to distinguish between positive class and negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.  Prediction with ***Neural Networks***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another prediction models that we implemented is Neural Networks model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = Path('models/neural_network.pkl')\n",
    "with open(path,'rb') as file:\n",
    "    model=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our predictions on the KNN model we first load the TEST dataset and then we differentiate the feautres (The first fifteen columns) with the target (TenYearCHD column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"datasets/test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test=df_test.copy(deep=True)\n",
    "X_test.drop('TenYearCHD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_test=df_test['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for Logistic Regression we start with the predictions output through the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(df_test['TenYearCHD']), predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('TRUE label')\n",
    "plt.xlabel('PREDICTED label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),ha='center',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the confusion matrix for the Neural Network model we cannot see how in the KNN an equal value for FP and FN. There is a variation that will lead to different values for precision, recall and F1-Score. Continuing the comparison with the KNN model we can see almost equal values for True Positive, True Negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Precision value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision=metric.precision_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The percentage of the accuracy of our positive predictions, represented by the Precision, is: ',\"{:.2%}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Precision value is lower than the Recall value and this is completely justified by the FP and FN values. In the precision we find the denominator FP which being higher than the FN give a smaller value than the Recall where we use the FN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Recall value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall=metric.recall_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The ratio of positive instances that are correctly detected by the classifier (true positive rate), represented by the Recall, is: ',\"{:.2%}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score=metric.f1_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The F1-Score, is: ',\"{:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1-Score represents the weighted average between Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The overall predicted accuracy of the model is: ',\"{:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc=roc_auc_score(np.array(df_test['TenYearCHD']), predictions)\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), predictions)\n",
    "plt.plot(figsize=(10,10))\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Neural Networks')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "print(' The Area Under the ROC curve (AUC) is: ', auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC Curve created for the Neural Networks model reports the trend of the True Positive Rate (Recall) with respect to the FP Rate. The resulting curve tells us how much model is capable of distinguishing between classes. The curve is located above the diagonal and this allows us to state that the model made is not bad. To give a more detailed explanation, let's analyze the AUC value which represents the area under the curve. For the Neural Networks model it is equal to about 0.66. This value is positive as it is higher than the 0.5 threshold which represents the worst case. When AUC is approximately 0.5, model has no discrimination capacity to distinguish between positive class and negative class. In our case we have 66% chance that model will be able to distinguish between positive class and negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(model.loss_curve_,label=\"train\")\n",
    "model_test = MLPClassifier(hidden_layer_sizes=model.hidden_layer_sizes, learning_rate_init=model.learning_rate_init, \n",
    "                           max_iter=model.max_iter, tol=model.tol)\n",
    "model_test.fit(X_test, target_test)\n",
    "plt.plot(model_test.loss_curve_,label=\"test\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot we reported the trend of the Loss curve with respect to the train dataset and the test dataset. Along the x axis I report the number of iterations performed while along the y axis I report the losses\n",
    "In our case the two curves tend downwards as the iterations increase and consequently to an increasingly smaller value for the losses. This  reduction means that the model is learning during the training performed. The Test loss decreases at the same rate for both curve for the first 200 iteration this means that the model generalizes very well in this first part.. The loss curve, after about 500 iterations, reaches a loss value of about 0.1/0.2 for both the test and the train. The lower the loss value, the more this means that the model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Predictions with *Decision Trees*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another, fairly simple, yet effective, method for classification we tried are Decision Tree classifiers (we are using \"decision_tree.pkl\", a file inside \"models\" folder of our repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path=Path('models/decision_tree.pkl')\n",
    "with open(path,'rb') as file:\n",
    "    model=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the **X_test** as our test-dataset with all the features, without the target column 'TenYearCHD':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test=df_test.copy(deep=True)\n",
    "X_test.drop('TenYearCHD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our target column will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target=df_test['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Depth selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees perform very differently at varying depths. While a very deep tree will be very certain in its decision making, it is also more prone to overfitting the data, so we will start by searching for the best maximum depth our tree should have.\n",
    "We will start with a tree of depth 1, and increase it, and see how it performs. \n",
    "\n",
    "We will use f1-score as our metric for this decision, but any other metric such as accuracy would work equally as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "heights = range(1, 25)\n",
    "f1_scores = []\n",
    "\n",
    "for i in heights:\n",
    "    tree = DecisionTreeClassifier(max_depth=i)\n",
    "    tree.fit(X_train, target_train)\n",
    "    pred = tree.predict(X_test)\n",
    "    f1_scores.append(metric.f1_score(target_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.plot(heights, f1_scores)\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()\n",
    "print(\"Best performing depth: \", np.argmax(f1_scores) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the model with the best performing depth for the rest of our metric calculations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=np.argmax(f1_scores) + 1)\n",
    "model.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the **predicted output**, using the predict() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and calculate the Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(df_test['TenYearCHD']), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('TRUE label')\n",
    "plt.xlabel('PREDICTED label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),ha='center',fontsize=18)\n",
    "plt.savefig('figures/DTree - CM.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps us to find the True Positives, True Negatives, False Positives an False Negatives of the prediction; in fact we can visualize:\n",
    " - rows as the **true** labels\n",
    " - coumns as the **predict** labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision=metric.precision_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The percentage of the accuracy of our positive predictions, represented by the Precision, is: ',\"{:.2%}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall=metric.recall_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The ratio of positive instances that are correctly detected by the classifier (true positive rate), represented by the Recall, is: ',\"{:.2%}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score=metric.f1_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The F1-Score, is: ',\"{:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It represents the **harmonic** mean of precision and recall.\n",
    "Instead of the regular mean (that gives equal weight to all values), harmonic mean gives more weight to low values,\n",
    "favoring classifiers that have similar precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the **predicted probabilities of getting the output as 1**, using the predict.proba() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate ROC and AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc=roc_auc_score(np.array(df_test['TenYearCHD']), predictions)\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), predictions)\n",
    "plt.plot(figsize=(19.2, 10.8))\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Decision Tree')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig('figures/DTree - ROC.png')\n",
    "plt.show()\n",
    "print(' The Area Under the ROC curve (AUC) is: ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curve, we are showing the ***true positive rate*** (recall) against the ***false positive rate*** (ratio of $0$ instances that are incorrectly classified as $1$, corresponding to $1 - specificity$ , where the specificity corresponds to the true negative rate, which is the ratio of $0$ instances that are correctly classified as $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ROC curve seems to suggest that the classifier is not so so bad... but we can extract more informations seeing the value of the AUC;\n",
    "in fact, we can accept AUC values that lie between 0.5 to 1, where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.\n",
    "\n",
    "Here, our Area Under the ROC Curve is equal to almost $0.70$ , so we can say **in general** that we obtained an acceptable discrimination... but **specifically** in contexts such as our case (medical diagnosis), very high (and higher than the result we obtained) AUCs are sought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end, we can say that the overall predicted **accuracy** of the model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The overall predicted accuracy of the model is: ',\"{:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, lesser depths can be better than a more fit model. Let's see if it holds for accuracy as it held for f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "for i in heights:\n",
    "    tree = DecisionTreeClassifier(max_depth=i)\n",
    "    tree.fit(X_train, target_train)\n",
    "    pred = tree.predict(X_test)\n",
    "    accuracies.append(metric.accuracy_score(target_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.plot(heights, accuracies)\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "print(\"Best performing depth: \", np.argmax(accuracies) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the best performing depth stayed the same, so we are fairly sure of our decision that the best tree for our data would have this depth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. The Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plot_tree(model)\n",
    "plt.savefig('figures/tree.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Predictions with *Random Forests*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from Decision Trees, Random Forests are an ensamble method that use less fit trees and majority voting to perform classification (we are using \"random_forest.pkl\", a file inside \"models\" folder of our repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path=Path('models/random_forest.pkl')\n",
    "with open(path,'rb') as file:\n",
    "    model=pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the **X_test** as our test-dataset with all the features, without the target column 'TenYearCHD':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test=df_test.copy(deep=True)\n",
    "X_test.drop('TenYearCHD',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so our target column will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target=df_test['TenYearCHD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Best number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with Decision Trees, a Random Forest Classifier performs very differently at varying depths, but unlike Decision Tree Classifiers, Random Forest Classifiers benefit from even less deep trees than the single tree. \n",
    "Another variable of note is the number of estimators. Having just one reduces the process to a decision tree. The more classifiers, the more \"votes\" for deciding the class there are, but also possibly more noise. \n",
    "We will keep the depth of the tree as a constant (the default best value for depth learned at the previous point), and vary the number of trees in the forest, ranging them from 1 to 100. \n",
    "\n",
    "As before, We will use f1-score as our metric for this decision, but any other metric such as accuracy would work equally as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "heights = range(1, 100)\n",
    "f1_scores = []\n",
    "\n",
    "for i in heights:\n",
    "    tree = RandomForestClassifier(n_estimators=i, max_depth=5)\n",
    "    tree.fit(X_train, target_train)\n",
    "    pred = tree.predict(X_test)\n",
    "    f1_scores.append(metric.f1_score(target_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.plot(heights, f1_scores)\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.show()\n",
    "print(\"Best performing number of estimators: \", np.argmax(f1_scores) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then save the model with the best performing depth for the rest of our metric calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=np.argmax(f1_scores) + 1, max_depth=5)\n",
    "model.fit(X_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the **predicted output**, using the predict() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and calculate the Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(np.array(df_test['TenYearCHD']), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['0','1']\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('TRUE label')\n",
    "plt.xlabel('PREDICTED label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),ha='center',fontsize=18)\n",
    "plt.savefig('figures/RF - CM.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps us to find the True Positives, True Negatives, False Positives an False Negatives of the prediction; in fact we can visualize:\n",
    " - rows as the **true** labels\n",
    " - coumns as the **predict** labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precision=metric.precision_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The percentage of the accuracy of our positive predictions, represented by the Precision, is: ',\"{:.2%}\".format(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "recall=metric.recall_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The ratio of positive instances that are correctly detected by the classifier (true positive rate), represented by the Recall, is: ',\"{:.2%}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score=metric.f1_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The F1-Score, is: ',\"{:.2%}\".format(f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It represents the **harmonic** mean of precision and recall.\n",
    "Instead of the regular mean (that gives equal weight to all values), harmonic mean gives more weight to low values,\n",
    "favoring classifiers that have similar precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Receiver Operating Characteristic (ROC) and Area Under the ROC Curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the **predicted probabilities of getting the output as 1**, using the predict.proba() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate ROC and AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "auc=roc_auc_score(np.array(df_test['TenYearCHD']), predictions)\n",
    "false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), predictions)\n",
    "plt.plot(figsize=(19.2, 10.8))\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Logistic regression')\n",
    "plt.plot(false_positive_rate, true_positive_rate)\n",
    "plt.plot([0, 1], ls=\"--\")\n",
    "plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.savefig('figures/RF - ROC.png')\n",
    "plt.show()\n",
    "print(' The Area Under the ROC curve (AUC) is: ', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the ROC curve, we are showing the ***true positive rate*** (recall) against the ***false positive rate*** (ratio of $0$ instances that are incorrectly classified as $1$, corresponding to $1 - specificity$ , where the specificity corresponds to the true negative rate, which is the ratio of $0$ instances that are correctly classified as $0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the ROC curve seems to suggest that the classifier is not so so bad... but we can extract more informations seeing the value of the AUC;\n",
    "in fact, we can accept AUC values that lie between 0.5 to 1, where 0.5 denotes a bad classifer and 1 denotes an excellent classifier.\n",
    "\n",
    "Here, our Area Under the ROC Curve is equal to almost $0.60$ , so we can say **in general** that we obtained an acceptable discrimination... but **specifically** in contexts such as our case (medical diagnosis), very high (and higher than the result we obtained) AUCs are sought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the end, we can say that the overall predicted **accuracy** of the model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracy=accuracy_score(target,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('The overall predicted accuracy of the model is: ',\"{:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, a different number of estimators can be better than more. Let's see if it holds for accuracy as it held for f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "for i in heights:\n",
    "    tree = RandomForestClassifier(n_estimators=i, max_depth=5)\n",
    "    tree.fit(X_train, target_train)\n",
    "    pred = tree.predict(X_test)\n",
    "    accuracies.append(metric.accuracy_score(target_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.plot(heights, accuracies)\n",
    "plt.xlabel('Depth of tree')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "print(\"Best performing number of estimators: \", np.argmax(accuracies) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will vary as at depths different from the highest, the trees will differ from one train to another. Using the same exact trained models as before should keep the same value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "f1_scores = []\n",
    "accuracies  = []\n",
    "\n",
    "plt.figure(figsize=(19.2, 10.8))\n",
    "for i, model in enumerate(Path('models').iterdir()):\n",
    "    with open(Path(model), 'rb') as m:\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        mod = pickle.load(m)\n",
    "        models.append(model.name.rstrip(model.suffix))\n",
    "        pred = mod.predict(X_test)\n",
    "        f1_scores.append(metric.f1_score(df_test['TenYearCHD'], pred))\n",
    "        accuracies.append(metric.accuracy_score(df_test['TenYearCHD'], pred))\n",
    "        probs = mod.predict_proba(X_test)[:,1]\n",
    "        area = roc_auc_score(df_test['TenYearCHD'], probs)\n",
    "        false_positive_rate, true_positive_rate, threshold = roc_curve(np.array(df_test['TenYearCHD']), probs)\n",
    "        plt.plot(figsize=(6.4, 5.4))\n",
    "        plt.title(model.name.rstrip(model.suffix) + '; ' + str(area))\n",
    "        plt.plot(false_positive_rate, true_positive_rate)\n",
    "        plt.plot([0, 1], ls=\"--\")\n",
    "        plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n",
    "        plt.ylabel('True Positive Rate (Recall)')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "plt.savefig('figures/ROComparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 and Accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting F1 and Accuracy scores, we will first scale the metrics, as to better show the difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "f1_scores = scaler.fit_transform(np.reshape(f1_scores, (-1, 1))).reshape(-1)\n",
    "accuracies = scaler.fit_transform(np.reshape(accuracies, (-1, 1))).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.bar(range(len(models)), f1_scores, tick_label=models)\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Scaled F1 Score')\n",
    "plt.savefig('figures/F1_Comp.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(19.2, 10.8))\n",
    "plt.bar(range(len(models)), accuracies, tick_label=models)\n",
    "plt.title('Accuracy Score Comparison')\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Scaled Accuracy Score')\n",
    "plt.savefig('figures/Acc_Comp.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Article from World Health Organization web-site: \n",
    "https://www.who.int/en/news-room/fact-sheets/detail/cardiovascular-diseases-(cvds)\n",
    "- DataSet resources:\n",
    "https://www.kaggle.com/dileep070/heart-disease-prediction-using-logistic-regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
